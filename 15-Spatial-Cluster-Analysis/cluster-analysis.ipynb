{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis\n",
    "\n",
    "Data reduction in statistics and machine learning: factor analysis, principle components analysis, cluster analysis\n",
    " - in statistics, several observed variables might represent a single latent variable (reduce number of dimensions)\n",
    " - in spatial analysis, several lat-long points may represent a single \"place\" (reduce number of examples)\n",
    "\n",
    "Cluster analysis algorithms\n",
    " - *k-means* partitions the data space into Voronoi tessellations. Translation: you give it a desired number of clusters and it will partition them into equal-sized clusters (minimizes variance and fails at density-based clustering).\n",
    " - *DBSCAN* discovers clusters as dense areas in space, surrounded by sparser areas. Points in the sparse areas are usually considered noise (needs there to be drop-offs in density to detect cluster borders).\n",
    " - *OPTICS* is similar to DBSCAN, but lets you find clusters of varying density.\n",
    " - many more...\n",
    "\n",
    "DBSCAN is appropriate for geospatial data (when using haversine/great-circle distances or projected data), and we'll focus on it today using scikit-learn.\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/clustering.html\n",
    "- http://scikit-learn.org/stable/modules/clustering.html#dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# magic command to display matplotlib plots inline within the ipython notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# import necessary modules\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, time\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of kilometers in one radian\n",
    "kms_per_radian = 6371.0088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: spatial clustering into groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load the data set\n",
    "df = pd.read_csv('data/summer-travel-gps-full.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# how many rows are in this data set?\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# scatterplot it to get a sense of what it looks like\n",
    "df = df.sort_values(by=['lat', 'lon'])\n",
    "ax = df.plot(kind='scatter', x='lon', y='lat', alpha=0.5, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DBSCAN\n",
    "\n",
    "The scikit-learn DBSCAN haversine distance metric requires data in the form of [latitude, longitude] and both inputs and outputs are in units of radians.\n",
    "\n",
    "- eps is the physical distance from each point that forms its neighborhood\n",
    "- min_samples is the min cluster size, otherwise it's noise - set to 1 so we get no noise\n",
    "\n",
    "Extract the lat, lon columns into a numpy matrix of coordinates, then convert to radians when you call fit, for use by scikit-learn's haversine metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent points consistently as (lat, lon)\n",
    "coords = df.as_matrix(columns=['lat', 'lon'])\n",
    "\n",
    "# define epsilon as 10 kilometers, converted to radians for use by haversine\n",
    "epsilon = 10 / kms_per_radian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "db = DBSCAN(eps=epsilon, min_samples=10, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "cluster_labels = db.labels_\n",
    "unique_labels = set(cluster_labels)\n",
    "\n",
    "# get the number of clusters\n",
    "num_clusters = len(set(cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get colors and plot all the points, color-coded by cluster (or gray if not in any cluster, aka noise)\n",
    "fig, ax = plt.subplots()\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "# for each cluster label and color, plot the cluster's points\n",
    "for cluster_label, color in zip(unique_labels, colors):\n",
    "    \n",
    "    size = 150\n",
    "    if cluster_label == -1: #make the noise (which is labeled -1) appear as smaller gray points\n",
    "        color = 'gray'\n",
    "        size = 30\n",
    "    \n",
    "    # plot the points that match the current cluster label\n",
    "    x_coords = coords[cluster_labels==cluster_label][:,1]\n",
    "    y_coords = coords[cluster_labels==cluster_label][:,0]\n",
    "    ax.scatter(x=x_coords, y=y_coords, c=color, edgecolor='k', s=size, alpha=0.5)\n",
    "\n",
    "ax.set_title('Number of clusters: {}'.format(num_clusters))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib offers lots of built-in colormaps: http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "The silhouette coefficient evaluates how close a point is to the other points in its cluster in comparison with how close it is to the points in the next nearest cluster. A high silhouette coefficient indicates the points are well-clustered and a low value indicates an outlier.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "coefficient = metrics.silhouette_score(coords, cluster_labels)\n",
    "print('Silhouette coefficient: {:0.03f}'.format(metrics.silhouette_score(coords, cluster_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you try: experiment with different epsilon values, minimum sizes, and colormaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: clustering to reduce data set size\n",
    "\n",
    "Rather than clustering to discover groups, I want to cluster to reduce the size of my data set. Even zoomed in very close, several locations have hundreds of data points stacked directly on top of each other due to the duration of time spent at one location. Unless we are interested in time dynamics, we simply do not need all of these spatially redundant points – they just bloat the data set’s size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set eps low (1.5km) so clusters are only formed by very close points\n",
    "epsilon = 1.5 / kms_per_radian\n",
    "\n",
    "# set min_samples to 1 so we get no noise - every point will be in a cluster even if it's a cluster of 1\n",
    "start_time = time.time()\n",
    "db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "cluster_labels = db.labels_\n",
    "unique_labels = set(cluster_labels)\n",
    "\n",
    "# get the number of clusters\n",
    "num_clusters = len(set(cluster_labels))\n",
    "\n",
    "# all done, print the outcome\n",
    "message = 'Clustered {:,} points down to {:,} clusters, for {:.1f}% compression in {:,.2f} seconds'\n",
    "print(message.format(len(df), num_clusters, 100*(1 - float(num_clusters) / len(df)), time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "coefficient = metrics.silhouette_score(coords, cluster_labels)\n",
    "print('Silhouette coefficient: {:0.03f}'.format(metrics.silhouette_score(coords, cluster_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# number of clusters, ignoring noise if present\n",
    "num_clusters = len(set(cluster_labels)) #- (1 if -1 in labels else 0)\n",
    "print('Number of clusters: {}'.format(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create a series to contain the clusters - each element in the series is the points that compose each cluster\n",
    "clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the point in each cluster that is closest to its centroid\n",
    "\n",
    "DBSCAN clusters may be non-convex. This technique just returns one representative point from each cluster. First get the lat,lon coordinates of the cluster's centroid (shapely represents the first coordinate in the tuple as x and the second as y, so lat is x and lon is y here). Then find the member of the cluster with the smallest great circle distance to the centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# given a cluster of points, return the point nearest to the cluster's centroid\n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geopy's great circle distance calculates the shortest distance between two points along the surface of a sphere. https://en.wikipedia.org/wiki/Great-circle_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# find the point in each cluster that is closest to its centroid\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "\n",
    "# unzip the list of centermost points (lat, lon) tuples into separate lat and lon lists\n",
    "lats, lons = zip(*centermost_points)\n",
    "\n",
    "# from these lats/lons create a new df of one representative point for each cluster\n",
    "representative_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "representative_points.tail()\n",
    "representative_points.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data set of 1,759 points has been reduced down to a spatially representative sample of 158 points. Last thing - grab the rows in the original dataframe that correspond to these 158 points (so that I keep city, country, and timestamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pull row from full data set (df) where lat/lon match the lat/lon of each row of representative points\n",
    "# use iloc[0] to pull just the first row if there are multiple matches\n",
    "rs = representative_points.apply(lambda row: df[(df['lat']==row['lat']) & (df['lon']==row['lon'])].iloc[0], axis=1)\n",
    "rs.to_csv('data/reduced-set.csv', index=False)\n",
    "rs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# to demonstrate the data reduction, compare how many observations of 'Spain' in each data set\n",
    "print(len(df[df['country']=='Spain']))\n",
    "print(len(rs[rs['country']=='Spain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the final reduced set of coordinate points vs the original full set\n",
    "fig, ax = plt.subplots(figsize=[10,7])\n",
    "rs_scatter = ax.scatter(rs['lon'], rs['lat'], c='m', alpha=0.3, s=200)\n",
    "df_scatter = ax.scatter(df['lon'], df['lat'], c='k', alpha=0.5, s=5)\n",
    "ax.set_title('Full data set vs DBSCAN reduced set')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend([df_scatter, rs_scatter], ['Full set', 'Reduced set'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the massive reduction in data set size, our smaller set is still spatially representative of the larger set (until you get to very fine spatial scales, as determined by the DBSCAN epsilon value).\n",
    "\n",
    "If you're interested in cluster analysis and want to explore further, advanced topics include:\n",
    " - R-trees: https://en.wikipedia.org/wiki/R-tree\n",
    " - k-d trees: https://en.wikipedia.org/wiki/K-d_tree\n",
    " - OPTICS: https://en.wikipedia.org/wiki/OPTICS_algorithm\n",
    " - ELKI, a Java-based tool, lets you use DBSCAN explicitly with lat-long distances: http://elki.dbs.ifi.lmu.de/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
